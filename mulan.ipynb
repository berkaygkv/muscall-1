{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berkayg/miniforge3/envs/muLan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-12 16:06:45 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ytid', 'start_s', 'end_s', 'audioset_positive_labels', 'aspect_list', 'caption', 'author_id', 'is_balanced_subset', 'is_audioset_eval'],\n",
       "    num_rows: 5521\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('google/MusicCaps', split='train')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def download_clip(\n",
    "    video_id,\n",
    "    output_file,\n",
    "    start_time,\n",
    "    end_time,\n",
    "    tmpdir='/tmp/musiccaps',\n",
    "    num_attempts=5,\n",
    "    url_base='https://youtube.com/watch?v='\n",
    "):\n",
    "    status = False\n",
    "\n",
    "    command = f\"\"\"yt-dlp --no-warnings -x --audio-format wav -f bestaudio -o \"{output_file}\" --download-sections \"*{start_time}-{end_time}\" {url_base}{video_id}\"\"\".strip()\n",
    "\n",
    "    attempts = 0\n",
    "    while True:\n",
    "        try:\n",
    "            output = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            attempts += 1\n",
    "            if attempts == num_attempts:\n",
    "                return status, err.output\n",
    "        # It's a try-except-else block. If there's no exception\n",
    "        # thrown then the else block is executed, i.e. if the video\n",
    "        # is successfully downloaded\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    status = os.path.exists(output_file)\n",
    "    return status, 'Downloaded'\n",
    "\n",
    "def process(example):\n",
    "    output_file = str(data_dir / f\"{example['ytid']}.wav\")\n",
    "    status = True\n",
    "    if not os.path.exists(output_file):\n",
    "        status = False\n",
    "        status, log = download_clip(\n",
    "            video_id=example['ytid'],\n",
    "            output_file=output_file,\n",
    "            start_time=example['start_s'],\n",
    "            end_time=example['end_s']\n",
    "        )\n",
    "\n",
    "    example['audio'] = output_file\n",
    "    example['downloaded_status'] = status\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "samples_to_load = 100\n",
    "cores = 4\n",
    "sampling_rate = 7700\n",
    "writer_batch_size = 1000\n",
    "data_dir = \"./music_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_tensor(example):\n",
    "    example[\"audio\"][\"array\"] = torch.from_numpy(example[\"audio\"][\"array\"]).to(torch.float32)\n",
    "    print(type(example[\"audio\"][\"array\"]))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.select(range(samples_to_load))\n",
    "\n",
    "data_dir = Path(data_dir)\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ds = ds.map(\n",
    "    process,\n",
    "    num_proc=cores,\n",
    "    writer_batch_size=writer_batch_size,\n",
    "    keep_in_memory=False\n",
    ").cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(numpy_to_tensor, num_proc=cores, writer_batch_size=writer_batch_size, keep_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ytid': '-0Gj8-vB1q4',\n",
       " 'start_s': 30,\n",
       " 'end_s': 40,\n",
       " 'audioset_positive_labels': '/m/0140xf,/m/02cjck,/m/04rlf',\n",
       " 'aspect_list': \"['low quality', 'sustained strings melody', 'soft female vocal', 'mellow piano melody', 'sad', 'soulful', 'ballad']\",\n",
       " 'caption': 'The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.',\n",
       " 'author_id': 4,\n",
       " 'is_balanced_subset': False,\n",
       " 'is_audioset_eval': True,\n",
       " 'audio': {'path': 'music_data/-0Gj8-vB1q4.wav',\n",
       "  'array': array([ 0.00042981,  0.00274746, -0.00614384, ..., -0.01440152,\n",
       "         -0.01727214, -0.02183699]),\n",
       "  'sampling_rate': 7700},\n",
       " 'downloaded_status': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('music_data')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "Path(\"music_data/-0Gj8-vB1q4.wav\").parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "muscall_data_dir = Path(\"./data/datasets\")\n",
    "audi_data_dir = Path(\"./music_data\")\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def update_data_json(track_data):\n",
    "    with open(muscall_data_dir.joinpath(\"data.json\"), \"r\") as rd:\n",
    "        data = json.load(rd)\n",
    "    ls = [track_data[\"audio_id\"] == k.get(\"audio_id\", \"\") for k in data]\n",
    "\n",
    "    if not any(ls):\n",
    "        data.append(track_data)\n",
    "        with open(muscall_data_dir.joinpath(\"data.json\"), \"w\") as wr:\n",
    "            data = json.dump(data, wr, indent=4)\n",
    "\n",
    "def export_data(sample, audio_id):\n",
    "    # sample[\"audio\"][\"array\"] = torch.from_numpy(sample[\"audio\"][\"array\"]).to(torch.float32)\n",
    "    array = sample[\"audio\"][\"array\"]\n",
    "    track_id = Path(sample[\"audio\"][\"path\"]).name.replace(\".wav\", \"\")\n",
    "    # audio_path = muscall_data_dir.joinpath(\"audio\", track_id).replace(\".wav\", \"\")\n",
    "    audio_path = muscall_data_dir.joinpath(\"audiocaption\", \"audio\", track_id.replace(\".wav\", \".npy\"))\n",
    "\n",
    "    caption = sample[\"caption\"]\n",
    "    update_data_json({\"audio_id\": audio_id, \"caption\": caption, \"audio_path\": str(audio_path)})\n",
    "    np.save(audio_path, array)\n",
    "# ds = ds.map(export_data, num_proc=cores, writer_batch_size=writer_batch_size, keep_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dt in enumerate(ds):\n",
    "    export_data(dt, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_size = len(os.listdir(\"data/datasets/audiocaption/audio\"))\n",
    "test_size = int(data_size * 0.1)\n",
    "validation_size = int(data_size * 0.1)\n",
    "train_size = data_size - (test_size + validation_size)\n",
    "with open(muscall_data_dir.joinpath(\"data.json\"), \"r\") as rd:\n",
    "    data = json.load(rd)\n",
    "\n",
    "test_data = []\n",
    "train_data = []\n",
    "validation_data = []\n",
    "for idx, dt in enumerate(data):\n",
    "    if idx < train_size:\n",
    "        train_data.append(dt)\n",
    "\n",
    "    elif train_size <= idx <  train_size + test_size:\n",
    "        test_data.append(dt)\n",
    "\n",
    "    elif train_size + test_size <= idx:\n",
    "        validation_data.append(dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(muscall_data_dir.joinpath(\"audiocaption\", \"dataset_train.json\"), \"w\") as wr:\n",
    "    json.dump(train_data, wr, indent=4)\n",
    "\n",
    "with open(muscall_data_dir.joinpath(\"audiocaption\", \"dataset_test.json\"), \"w\") as wr:\n",
    "    json.dump(test_data, wr, indent=4)\n",
    "\n",
    "with open(muscall_data_dir.joinpath(\"audiocaption\", \"dataset_val.json\"), \"w\") as wr:\n",
    "    json.dump(validation_data, wr, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"torch\", columns=[\"caption\", \"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'caption': 'The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.',\n",
       " 'audio': {'path': 'music_data/-0Gj8-vB1q4.wav',\n",
       "  'array': tensor([ 0.0004,  0.0027, -0.0061,  ..., -0.0144, -0.0173, -0.0218]),\n",
       "  'sampling_rate': tensor(7700)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicapsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds):\n",
    "        super(MusicapsDataset, self).__init__()\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (ds[idx][\"audio\"][\"array\"], ds[idx][\"caption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_ds = MusicapsDataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0004,  0.0027, -0.0062,  ..., -0.0144, -0.0173, -0.0219]),\n",
       " 'The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    arrays = [x[0] for x in data]\n",
    "    captions = [x[1] for x in data]\n",
    "    maxlen_array = 0\n",
    "    for array in arrays:\n",
    "        if len(array) > maxlen_array:\n",
    "            maxlen_array = len(array)\n",
    "    for i in range(len(data)):\n",
    "        arrays[i] = torch.cat([arrays[i], torch.zeros(size=(maxlen_array - len(arrays[i]),))])\n",
    "    arrays = torch.stack(arrays)\n",
    "    return arrays, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transformer = AudioSpectrogramTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    spec_n_fft = 128,\n",
    "    spec_win_length = 24,\n",
    "    spec_aug_stretch_factor = 0.8\n",
    ")\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "mulan = MuLaN(\n",
    "    audio_transformer = audio_transformer,\n",
    "    text_transformer = text_transformer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiclm_pytorch import MuLaNTrainer\n",
    "mulan_trainer = MuLaNTrainer(\n",
    "    mulan=mulan,\n",
    "    dataset=mod_ds,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "mulan_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays:\n",
      "torch.Size([3, 153796])\n",
      "Captions:\n",
      "['The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.', 'This song features an electric guitar as the main instrument. The guitar plays a descending run in the beginning then plays an arpeggiated chord followed by a double stop hammer on to a higher note and a descending slide followed by a descending chord run. The percussion plays a simple beat using rim shots. The percussion plays in common time. The bass plays only one note on the first count of each bar. The piano plays backing chords. There are no voices in this song. The mood of this song is relaxing. This song can be played in a coffee shop.', 'a male voice is singing a melody with changing tempos while snipping his fingers rhythmically. The recording sounds like it has been recorded in an empty room. This song may be playing, practicing snipping and singing along.']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=mod_ds, batch_size=16, collate_fn=custom_collate)\n",
    "for arrays, captions in train_dataloader:\n",
    "    print(f\"Arrays:\\n{arrays.shape}\")\n",
    "    print(f\"Captions:\\n{captions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 153796])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_ds[0][0].reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# get a ton of <sound, text> pairs and train\n",
    "\n",
    "wavs = torch.randn(2, 1024)\n",
    "wavs = mod_ds[0][0].reshape(1, -1)\n",
    "texts = torch.randint(0, 20000, (2, 256))\n",
    "raw_texts = [\"The low quality recording features a ballad song\", \"This song features an electric guitar\"]\n",
    "\n",
    "loss = mulan(wavs, raw_texts=raw_texts)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.6719, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after much training, you can embed sounds and text into a joint embedding space\n",
    "# for conditioning the audio LM\n",
    "\n",
    "embeds = mulan.get_audio_latents(wavs)  # during training\n",
    "\n",
    "embeds = mulan.get_text_latents(texts)  # during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 114kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.62MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.39MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.01MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# For more details - https://huggingface.co/bert-base-uncased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.\n"
     ]
    }
   ],
   "source": [
    "print(captions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"'The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.\")\n",
    "caption_tokens = tokenizer(captions, return_tensors=\"np\", padding=\"max_length\", max_length=256, truncation=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_latents = mulan.get_audio_latents(arrays)\n",
    "audio_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "raw_texts = [\"The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.\"]\n",
    "# raw_texts = [\"happy music\"]\n",
    "text_latents = mulan.get_text_latents(raw_texts=raw_texts)\n",
    "print(text_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3225],\n",
       "        [0.3253],\n",
       "        [0.3522]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_audio_text = audio_latents @ text_latents.T\n",
    "logits_audio_text.softmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['audio.to_patch_tokens.1.weight', 'audio.to_patch_tokens.1.bias', 'audio.to_patch_tokens.2.weight', 'audio.to_patch_tokens.2.bias', 'audio.to_patch_tokens.3.weight', 'audio.to_patch_tokens.3.bias', 'audio.spec.window', 'audio.aug.0.phase_advance', 'audio.transformer.layers.0.0.q_scale', 'audio.transformer.layers.0.0.k_scale', 'audio.transformer.layers.0.0.norm.learned_gamma', 'audio.transformer.layers.0.0.to_q.weight', 'audio.transformer.layers.0.0.to_kv.weight', 'audio.transformer.layers.0.0.to_out.0.weight', 'audio.transformer.layers.0.1.0.learned_gamma', 'audio.transformer.layers.0.1.1.weight', 'audio.transformer.layers.0.1.4.weight', 'audio.transformer.layers.1.0.q_scale', 'audio.transformer.layers.1.0.k_scale', 'audio.transformer.layers.1.0.norm.learned_gamma', 'audio.transformer.layers.1.0.to_q.weight', 'audio.transformer.layers.1.0.to_kv.weight', 'audio.transformer.layers.1.0.to_out.0.weight', 'audio.transformer.layers.1.1.0.learned_gamma', 'audio.transformer.layers.1.1.1.weight', 'audio.transformer.layers.1.1.4.weight', 'audio.transformer.layers.2.0.q_scale', 'audio.transformer.layers.2.0.k_scale', 'audio.transformer.layers.2.0.norm.learned_gamma', 'audio.transformer.layers.2.0.to_q.weight', 'audio.transformer.layers.2.0.to_kv.weight', 'audio.transformer.layers.2.0.to_out.0.weight', 'audio.transformer.layers.2.1.0.learned_gamma', 'audio.transformer.layers.2.1.1.weight', 'audio.transformer.layers.2.1.4.weight', 'audio.transformer.layers.3.0.q_scale', 'audio.transformer.layers.3.0.k_scale', 'audio.transformer.layers.3.0.norm.learned_gamma', 'audio.transformer.layers.3.0.to_q.weight', 'audio.transformer.layers.3.0.to_kv.weight', 'audio.transformer.layers.3.0.to_out.0.weight', 'audio.transformer.layers.3.1.0.learned_gamma', 'audio.transformer.layers.3.1.1.weight', 'audio.transformer.layers.3.1.4.weight', 'audio.transformer.layers.4.0.q_scale', 'audio.transformer.layers.4.0.k_scale', 'audio.transformer.layers.4.0.norm.learned_gamma', 'audio.transformer.layers.4.0.to_q.weight', 'audio.transformer.layers.4.0.to_kv.weight', 'audio.transformer.layers.4.0.to_out.0.weight', 'audio.transformer.layers.4.1.0.learned_gamma', 'audio.transformer.layers.4.1.1.weight', 'audio.transformer.layers.4.1.4.weight', 'audio.transformer.layers.5.0.q_scale', 'audio.transformer.layers.5.0.k_scale', 'audio.transformer.layers.5.0.norm.learned_gamma', 'audio.transformer.layers.5.0.to_q.weight', 'audio.transformer.layers.5.0.to_kv.weight', 'audio.transformer.layers.5.0.to_out.0.weight', 'audio.transformer.layers.5.1.0.learned_gamma', 'audio.transformer.layers.5.1.1.weight', 'audio.transformer.layers.5.1.4.weight', 'audio.norm.learned_gamma', 'audio.dynamic_pos_bias_mlp.0.weight', 'audio.dynamic_pos_bias_mlp.0.bias', 'audio.dynamic_pos_bias_mlp.2.weight', 'audio.dynamic_pos_bias_mlp.2.bias', 'audio.dynamic_pos_bias_mlp.4.weight', 'audio.dynamic_pos_bias_mlp.4.bias', 'text.cls_token', 'text.token_emb.weight', 'text.pos_emb.weight', 'text.transformer.layers.0.0.q_scale', 'text.transformer.layers.0.0.k_scale', 'text.transformer.layers.0.0.norm.learned_gamma', 'text.transformer.layers.0.0.to_q.weight', 'text.transformer.layers.0.0.to_kv.weight', 'text.transformer.layers.0.0.to_out.0.weight', 'text.transformer.layers.0.1.0.learned_gamma', 'text.transformer.layers.0.1.1.weight', 'text.transformer.layers.0.1.4.weight', 'text.transformer.layers.1.0.q_scale', 'text.transformer.layers.1.0.k_scale', 'text.transformer.layers.1.0.norm.learned_gamma', 'text.transformer.layers.1.0.to_q.weight', 'text.transformer.layers.1.0.to_kv.weight', 'text.transformer.layers.1.0.to_out.0.weight', 'text.transformer.layers.1.1.0.learned_gamma', 'text.transformer.layers.1.1.1.weight', 'text.transformer.layers.1.1.4.weight', 'text.transformer.layers.2.0.q_scale', 'text.transformer.layers.2.0.k_scale', 'text.transformer.layers.2.0.norm.learned_gamma', 'text.transformer.layers.2.0.to_q.weight', 'text.transformer.layers.2.0.to_kv.weight', 'text.transformer.layers.2.0.to_out.0.weight', 'text.transformer.layers.2.1.0.learned_gamma', 'text.transformer.layers.2.1.1.weight', 'text.transformer.layers.2.1.4.weight', 'text.transformer.layers.3.0.q_scale', 'text.transformer.layers.3.0.k_scale', 'text.transformer.layers.3.0.norm.learned_gamma', 'text.transformer.layers.3.0.to_q.weight', 'text.transformer.layers.3.0.to_kv.weight', 'text.transformer.layers.3.0.to_out.0.weight', 'text.transformer.layers.3.1.0.learned_gamma', 'text.transformer.layers.3.1.1.weight', 'text.transformer.layers.3.1.4.weight', 'text.transformer.layers.4.0.q_scale', 'text.transformer.layers.4.0.k_scale', 'text.transformer.layers.4.0.norm.learned_gamma', 'text.transformer.layers.4.0.to_q.weight', 'text.transformer.layers.4.0.to_kv.weight', 'text.transformer.layers.4.0.to_out.0.weight', 'text.transformer.layers.4.1.0.learned_gamma', 'text.transformer.layers.4.1.1.weight', 'text.transformer.layers.4.1.4.weight', 'text.transformer.layers.5.0.q_scale', 'text.transformer.layers.5.0.k_scale', 'text.transformer.layers.5.0.norm.learned_gamma', 'text.transformer.layers.5.0.to_q.weight', 'text.transformer.layers.5.0.to_kv.weight', 'text.transformer.layers.5.0.to_out.0.weight', 'text.transformer.layers.5.1.0.learned_gamma', 'text.transformer.layers.5.1.1.weight', 'text.transformer.layers.5.1.4.weight', 'text.norm.learned_gamma', 'text_to_latents.weight', 'text_to_latents.bias', 'audio_to_latents.weight', 'audio_to_latents.bias', 'contrast.temperatures'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MuLaN(\n",
       "  (audio): AudioSpectrogramTransformer(\n",
       "    (to_patch_tokens): Sequential(\n",
       "      (0): Rearrange('b (h p1) (w p2) -> b h w (p1 p2)', p1=16, p2=16)\n",
       "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (spec): Spectrogram()\n",
       "    (aug): Sequential(\n",
       "      (0): TimeStretch()\n",
       "      (1): FrequencyMasking()\n",
       "      (2): TimeMasking()\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "            (2): GEGLU()\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "    (dynamic_pos_bias_mlp): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): SiLU()\n",
       "      (4): Linear(in_features=128, out_features=8, bias=True)\n",
       "      (5): Rearrange('... i j h -> ... h i j')\n",
       "    )\n",
       "  )\n",
       "  (text): TextTransformer(\n",
       "    (token_emb): Embedding(49408, 512)\n",
       "    (pos_emb): Embedding(256, 512)\n",
       "    (transformer): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "            (2): GEGLU()\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (text_to_latents): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (audio_to_latents): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (contrast): SoftmaxContrastiveLearning(\n",
       "    (all_gather): AllGather()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MuLaN(\n",
    "    audio_transformer = audio_transformer,\n",
    "    text_transformer = text_transformer\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\"/Users/berkayg/Codes/music-project/muscall/mulan.1.pt\", map_location=torch.device('mps'))\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x2a512bce0>\n",
       "[\n",
       "  [\n",
       "    \"-0Gj8-vB1q4\"\n",
       "  ],\n",
       "  [\n",
       "    \"-0SdAVK79lg\"\n",
       "  ],\n",
       "  [\n",
       "    \"-0vPFx-wRRI\"\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data[\"ytid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "audio_latents = model.get_audio_latents(arrays)\n",
    "audio_latents.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1528],\n",
       "        [0.1335],\n",
       "        [0.0916]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_audio_text = audio_latents @ text_latents.T\n",
    "logits_audio_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.mps.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-tagging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
